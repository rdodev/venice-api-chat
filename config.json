{
  "api": {
    "endpoint": "https://api.venice.ai/api/v1/chat/completions",
    "model": "llama-3.2-3b",
    "stream": true,
    "max_tokens": 1000,
    "temperature": 0.7,
    "top_p": 1,
    "frequency_penalty": 0,
    "presence_penalty": 0,
    "venice_parameters": {
      "include_venice_system_prompt": false
    }
  },
  "server": {
    "port": 3000,
    "host": "0.0.0.0"
  }
}